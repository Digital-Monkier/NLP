{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Simple Transformer Script Summarization App.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNFnpdFSRPhtCKkoWrtOnTU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Streamlit Based Script Summarization App\n","- We will use a blend of standard transformer archetectures and piplines from huggingface in combination with the awesome stremlit library to create a text summaraization app which can handle any length of text\n","- We will also attempt to tailor it to film and tv script formatting which is very distinct and rather awkward for parsing.\n","\n","- We will use a selection of scripts sourced online to test our application which are freely available online and I have left an open link to a foilder containing them\n","  - Breaking Bad Pilot\n","  - Single Drunk Female Pilot\n","  - Better Call Saul Seaon 1 Episode 6\n"],"metadata":{"id":"Cc9iTTeJpPWE"}},{"cell_type":"code","source":["!gdown --id gUlZLnaYrjvbfe-A7jCikxuMXddiYY3I"],"metadata":{"id":"mSNo5u-Q3vZT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Take Aways\n","### Pros\n","- We managed to clean the scripts really well by using a brute force approach\n","  - Regex removal of dialogue \n","  - Brute force regex removal of all all numbers and special characters\n","  - Reformatting paragraph structure using a base transformer model\n","  - Added the option to enter the page number where the story content begins removing the summarization of script table of contents etc...\n","- We also created a short function which builds paragraph chunks of a set amount of tokens upto max 500 tokens. Enabling us to play around with summarizing different amounts of text at a time\n","  - Using the upper limit seemed to work the best in this instance\n","\n","### Cons\n","- The summary quality is lacking in some regards as parsing the entire script in randomized chunks (midway through a scene or from the end of one scene to the tsrat of another) is not the ideal process\n","- We will improve on this model by researching general script stuctures and begin to count out the pages and sectioning the acts more appropriatly we can then summarize given parts of each scene or act and feed the model the scenes in a more targeted manner\n","\n","# Next Steps \n","### Research & Restructure Chunks\n","- As mentioned above I will continue this project by studying general act and scene formatting across the 20-30 / 60 and 120 min script formats\n","- I will look for a way to consitently isolate the correct acts/scenes to feed the more with a more targeted methodology\n","- In the event there is not triggers for the sectioning I will create a fall back based on the average page lengths for scenes "],"metadata":{"id":"-r1VCGEhyQJJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QsTGHPmncb50"},"outputs":[],"source":["%%capture\n","!pip --q install streamlit\n","!pip --q install transformers\n","!pip --q install PyPDF2\n","!pip --q install docx2txt\n","!pip --q install deepmultilingualpunctuation\n","!pip --q install sentencepiece"]},{"cell_type":"code","source":["# Import them\n","from PyPDF2 import PdfFileReader\n","import numpy as np\n","import seaborn as sns\n","import re\n","from deepmultilingualpunctuation import PunctuationModel\n"],"metadata":{"id":"36BJxjbYa6xl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Testing Space"],"metadata":{"id":"6peEm0jC0G3u"}},{"cell_type":"code","source":["# model = PunctuationModel()\n","\n","# # load\n","# start_page = 3\n","# def read_pdf(file):\n","#   pdfReader = PdfFileReader(file)\n","#   count = pdfReader.numPages\n","#   all_page_text = \"\"\n","#   for i in range(count - start_page):\n","#     page = pdfReader.getPage(start_page+i)\n","#     all_page_text += page.extractText()\n","#   return all_page_text\n","\n","\n","# text = read_pdf(\"Better_Call_Saul_1x06_-_Five-O.pdf\")\n","# # start_idx = re.search(\"INT|EXT\", text).start()\n","# # text = text[start_idx:]\n","\n","# # All Dialogue\n","# match = re.compile(r\"(?m)^\\s*\\b([A-Z]+)\\b\\s*\\n(.*(?:\\n.+)*)\")\n","# text = match.sub(r' ',text)\n","\n","# # Special script preprocessing in brackets and scene setting\n","# text = re.sub(\"[^a-zA-Z']|INT|EXT\",\" \", text)\n","\n","# # Standard preprocess (lower/remove newline/spaces)\n","# text = text.lower()\n","\n","# # Remove the header text from the script\n","# title_text =\"Better_Call_Saul_1x06_-_Five-O.pdf\"\n","# raw_tokens = re.split(\"[^A-Za-z0-9-]\", title_text.lower())\n","# title_tokens = [t for t in raw_tokens if t]\n","# text = re.sub(\"|\".join(title_tokens), \"\", text)\n","# text = ' '.join(text.split())\n","# text = model.restore_punctuation(text)\n","\n","# # Lets save it at this stage first\n","# with open('BCS_Clean.txt', 'w') as f_out:\n","#     f_out.write(text)"],"metadata":{"id":"Wlu7Msbtladd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# text"],"metadata":{"id":"X71yrfxHwXhZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sentences = text.split(\".\")\n","\n","# max_chunk = 350\n","# list_index = 0 \n","# chunks = []\n","# for sent in sentences:\n","#     if len(chunks) == list_index + 1: \n","#         if len(chunks[list_index]) + len(sent.split(' ')) <= max_chunk:\n","#             chunks[list_index].extend(sent.split(' '))\n","#         else:\n","#             list_index += 1\n","#             chunks.append(sent.split(' '))\n","#     else:\n","#         chunks.append(sent.split(' '))\n","\n","# for chunk_id in range(len(chunks)):\n","#     chunks[chunk_id] = ' '.join(chunks[chunk_id])\n"],"metadata":{"id":"b3QJWlt-HKdM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Streamlit Summarization App"],"metadata":{"id":"wyQI8Fan0LsU"}},{"cell_type":"code","source":["%%writefile app.py\n","import streamlit as st\n","from transformers import pipeline\n","import sentencepiece\n","import re\n","from PyPDF2 import PdfFileReader\n","import docx2txt\n","from deepmultilingualpunctuation import PunctuationModel\n","\n","\n","def load_puntuator():\n","    punc_model = PunctuationModel()\n","    return punc_model\n","\n","def load_summarizer(model):\n","    model = pipeline(\"summarization\", model=model, device=0)\n","    return model\n","\n","def parse_script(raw_input):\n","    # Removing Dialogue\n","    match = re.compile(r\"(?m)^\\s*\\b([A-Z]+)\\b\\s*\\n(.*(?:\\n.+)*)\")\n","    raw_input = match.sub(r' ',raw_input)\n","    # Removing all non-word characters (excl \"'\") and standard scene descriptors\n","    raw_input = re.sub(\"[^a-zA-Z']|INT|EXT\",\" \", raw_input)\n","    # Lowercasing text\n","    raw_input = raw_input.lower()\n","    # Remove all or most of the page header text\n","    title_text =\"Better_Call_Saul_1x06_-_Five-O.pdf\"\n","    raw_tokens = re.split(\"[^A-Za-z0-9-]\", title_text.lower())\n","    title_tokens = [t for t in raw_tokens if t]\n","    raw_input = re.sub(\"|\".join(title_tokens), \"\", raw_input)\n","    # Split and join to remove empty spaces \n","    processed_text = ' '.join(raw_input.split())\n","    # Re punc output\n","    punctuated_text = punctuator.restore_punctuation(processed_text)\n","    # Tokenize ahead of paragraphing\n","    sentences = punctuated_text.split(\".\")\n","    return sentences\n","\n","\n","def create_paragraphs(sentences):\n","    max_chunk = 256\n","    list_index = 0 \n","    chunks = []\n","    for sent in sentences:\n","        if len(chunks) == list_index + 1: \n","            if len(chunks[list_index]) + len(sent.split(' ')) <= max_chunk:\n","                chunks[list_index].extend(sent.split(' '))\n","            else:\n","                list_index += 1\n","                chunks.append(sent.split(' '))\n","        else:\n","            chunks.append(sent.split(' '))\n","\n","    for chunk_id in range(len(chunks)):\n","        chunks[chunk_id] = ' '.join(chunks[chunk_id])\n","    return chunks\n","\n","\n","# Streamlit App Construction\n","st.title(\"Script Summarizer\")\n","\n","max = st.sidebar.slider('Select max', 50, 250, step=10, value=150)\n","min = st.sidebar.slider('Select min', 10, 100, step=10, value=50)\n","\n","model_option = st.selectbox(\n","     'Select summarization model',\n","     (\"\",\n","      \"deep-learning-analytics/wikihow-t5-small\",\n","      \"facebook/bart-large-cnn\", \n","      \"google/pegasus-xsum\", \n","      \"sshleifer/distilbart-cnn-6-6\", \n","      \"t5-large\", \n","      \"google/pegasus-large\"))\n","\n","if model_option is not \"\":\n","  summarizer = load_summarizer(model_option)\n","  punctuator = load_puntuator()\n","\n","\n","uploaded_file = st.file_uploader(\n","                \"Upload your pdf file\", \n","                type=[\"pdf\", \"docx\"],\n","                accept_multiple_files=False\n",")\n","\n","start_page = st.text_input('Enter the page number where the story begins', '')\n","\n","file_button = st.button(\"Summarize File\")\n","\n","# Reading Different File Types\n","def read_pdf(file):\n","  pdfReader = PdfFileReader(file)\n","  count = pdfReader.numPages\n","  all_page_text = \"\"\n","  for i in range(count - int(start_page)):\n","    page = pdfReader.getPage(int(start_page) + i)\n","    all_page_text += page.extractText()\n","  return all_page_text\n"," \n","if file_button and uploaded_file is not None:\n","    if uploaded_file.name[-4:] == 'docx':\n","      raw_input = docx2txt.process(uploaded_file)\n","\n","    elif uploaded_file.name[-3:] == 'pdf':\n","      raw_input = read_pdf(uploaded_file)\n","      \n","with st.spinner(\"Generating Summary..\"):\n","  if file_button and raw_input:\n","        sentences = parse_script(raw_input)\n","        punct_paragraphs = create_paragraphs(sentences)\n","        res = summarizer(punct_paragraphs,\n","                        max_length=max, \n","                        min_length=min)\n","        text = ' '.join([summ['summary_text'] for summ in res])\n","        st.write(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KcYBTRusAdcu","executionInfo":{"status":"ok","timestamp":1647356699128,"user_tz":-120,"elapsed":478,"user":{"displayName":"Darragh Caffrey","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15949162394673566308"}},"outputId":"5fc13d3e-8d47-4f32-d9a8-bec2d9fb9320"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"code","source":["!streamlit run app.py & npx localtunnel --port 8501"],"metadata":{"id":"HV101LJUv7Kw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Conclusion\n","- We lose a little summary context by removing the periods from the sentences making some of the paragphs have an odd start/finish\n","- On the contrary not removing them creates too much noise as the format of the script have short punctuated sentence\n","\n","\n","\n","# Further Development\n","- An algorithm trained to insert punctuation would be ideal here to transform the raw structure the cleaned scene decriptions into a story and from there parse it into chunks whose begining and ending is alligned with a sentence structure"],"metadata":{"id":"3GyiIiMshFkT"}}]}